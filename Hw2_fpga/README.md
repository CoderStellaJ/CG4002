### Installation of FINN Compiler on Ultra96
This is required as we will be utilizing the finn core and finn onnx library to run onnx model on the Ultra96

- git clone <https://github.com/Xilinx/finn.git>

- cd into finn base directory

- pip3 install .

Install other required modules with PIP

### ONNXRUNTIME INSTALLATION STEP
Install onnxruntime with pre-built wheel for aarch64 and python3.6

### ONNXRUNTIME BUILD STEP
Visit onnxruntime github repository v1.2.0 at <https://github.com/microsoft/onnxruntime/tree/v1.2.0>

Follow the instructions in BUILD.md to build onnxruntime from source on an ARM architecture

- Upload dataflow parent onnx model and remote execution onnx model generated by the finn compiler e2e example notebook into the same directory on the Ultra96.
- The bitstream file, tcl file, hw_accelerate_main.py, fpga.py should also be in the same directory.
- custom_onnx_exec.py can also be used if required. But it is only required if you definitely have to use onnxruntime

To hardware accelerate your neural network:

```python
import hw_accelerate_main
parent_model = hw_accelerate_parent_model_setup():
output_numpy_array = hw_accelerate_parent_model_eval(parent_model, input_numpy_array)
```

### OTHER NOTES WHEN USING FINN

The finn R paper says that the first and last layers of QNNs are left quantized sensitive.

However, a modest direct quantization to 8-bit fixed point quantities have little
to no impact on accuracy and could leads to significant resource savings.

As of the time of writing, FINN does not generate a bitstream with input quantization. A software approach is required.
